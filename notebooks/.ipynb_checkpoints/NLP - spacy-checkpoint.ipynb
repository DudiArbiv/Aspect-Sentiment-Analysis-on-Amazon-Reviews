{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/apple/Documents/gatech/dva_6242/project/notebooks/data/raw/amazon_reviews_us_Electronics_v1_00.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-827c23964a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maspect_extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0maspect_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/gatech/dva_6242/project/src/models/aspect_extraction.py\u001b[0m in \u001b[0;36maspect_extraction\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/raw/amazon_reviews_us_Electronics_v1_00.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfetch_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0maspect_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_aspects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gatech/dva_6242/project/src/models/aspect_extraction.py\u001b[0m in \u001b[0;36mfetch_reviews\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dva/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dva/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dva/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dva/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dva/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/apple/Documents/gatech/dva_6242/project/notebooks/data/raw/amazon_reviews_us_Electronics_v1_00.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "# from src.models import aspect_extraction\n",
    "\n",
    "# aspect_extraction.aspect_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get this from some place else. This is just for dummy usage\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "            \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \n",
    "            \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "            \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \n",
    "            \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \n",
    "            \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \n",
    "            \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \n",
    "            \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "            \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.abspath(os.path.join('..')) # base path of project\n",
    "\n",
    "file_path = BASE_PATH + '/data/raw/amazon_reviews_us_Electronics_v1_00.tsv'\n",
    "\n",
    "raw_data = pd.read_table(file_path,error_bad_lines=False, nrows=100)\n",
    "\n",
    "#review_body = raw_data['review_body']\n",
    "reviews = raw_data[['review_id', 'review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspects(x):\n",
    "    doc=nlp(x) \n",
    "    doc=[i.text for i in doc if i.text not in stop_words and i.pos_==\"NOUN\"] ## Remove common words and retain only nouns\n",
    "    doc=list(map(lambda i: i.lower(),doc)) ## Normalize text to lower case\n",
    "    doc=pd.Series(doc)\n",
    "    doc=doc.value_counts().head().index.tolist() ## Get 5 most frequent nouns\n",
    "    return doc\n",
    "\n",
    "aspects = []\n",
    "\n",
    "for review in review_body :\n",
    "    aspects.append(get_aspects(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT det DET\n",
      "sound JJ amod ADJ\n",
      "quality NN nsubj NOUN\n",
      "of IN prep ADP\n",
      "the DT det DET\n",
      "speakers NNS pobj NOUN\n",
      "are VBP ROOT VERB\n",
      "wonderful JJ acomp ADJ\n",
      ". . punct PUNCT\n",
      "However RB advmod ADV\n",
      ", , punct PUNCT\n",
      "the DT det DET\n",
      "packaging NN nsubj NOUN\n",
      "could MD aux VERB\n",
      "have VB aux VERB\n",
      "been VBN ROOT VERB\n",
      "better JJR acomp ADJ\n",
      ". . punct PUNCT\n",
      "Photos NNS nsubj NOUN\n",
      "under IN prep ADP\n",
      "low JJ amod ADJ\n",
      "lighting NN pobj NOUN\n",
      "is VBZ ROOT VERB\n",
      "poor JJ amod ADJ\n",
      "- : punct PUNCT\n",
      "both DT preconj DET\n",
      "front NN amod NOUN\n",
      "and CC cc CCONJ\n",
      "back NN conj NOUN\n",
      "cameras NNS attr NOUN\n"
     ]
    }
   ],
   "source": [
    "dummy = \"The sound quality of the speakers are wonderful. However, the packaging could have been better. Photos under low lighting is poor - both front and back cameras\"\n",
    "doc=nlp(dummy) \n",
    "for token in doc:\n",
    "    print(token.text,token.tag_, token.dep_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sound quality  *** quality  *** nsubj  *** are\n",
      "the speakers  *** speakers  *** pobj  *** of\n",
      "the packaging  *** packaging  *** nsubj  *** been\n",
      "Photos  *** Photos  *** nsubj  *** is\n",
      "low lighting  *** lighting  *** pobj  *** under\n",
      "poor - both front and back cameras  *** cameras  *** attr  *** is\n"
     ]
    }
   ],
   "source": [
    "#noun chunks\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,\" ***\", chunk.root.text, \" ***\",chunk.root.dep_,\" ***\", chunk.root.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The **** det **** quality **** False NOUN []\n",
      "sound **** amod **** quality **** False NOUN []\n",
      "quality **** nsubj **** are **** False VERB [The, sound, of]\n",
      "of **** prep **** quality **** False NOUN [speakers]\n",
      "the **** det **** speakers **** False NOUN []\n",
      "speakers **** pobj **** of **** False ADP [the]\n",
      "are **** ROOT **** are **** False VERB [quality, wonderful, .]\n",
      "wonderful **** acomp **** are **** False VERB []\n",
      ". **** punct **** are **** False VERB []\n",
      "However **** advmod **** been **** False VERB []\n",
      ", **** punct **** been **** False VERB []\n",
      "the **** det **** packaging **** False NOUN []\n",
      "packaging **** nsubj **** been **** False VERB [the]\n",
      "could **** aux **** been **** False VERB []\n",
      "have **** aux **** been **** False VERB []\n",
      "been **** ROOT **** been **** False VERB [However, ,, packaging, could, have, better, .]\n",
      "better **** acomp **** been **** False VERB []\n",
      ". **** punct **** been **** False VERB []\n",
      "Photos **** nsubj **** is **** False VERB [under]\n",
      "under **** prep **** Photos **** False NOUN [lighting]\n",
      "low **** amod **** lighting **** False NOUN []\n",
      "lighting **** pobj **** under **** False ADP [low]\n",
      "is **** ROOT **** is **** False VERB [Photos, cameras]\n",
      "poor **** amod **** front **** False NOUN [-]\n",
      "- **** punct **** poor **** False ADJ []\n",
      "both **** preconj **** front **** False NOUN []\n",
      "front **** amod **** cameras **** False NOUN [poor, both, and, back]\n",
      "and **** cc **** front **** False NOUN []\n",
      "back **** conj **** front **** False NOUN []\n",
      "cameras **** attr **** is **** False VERB [front]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\"****\" ,token.dep_, \"****\" ,token.head.text, \"****\" ,token.is_stop,token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Nov/2018 19:18:46] \"GET / HTTP/1.1\" 200 18731\n",
      "127.0.0.1 - - [11/Nov/2018 19:18:47] \"GET / HTTP/1.1\" 200 18731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this if you want to visualise dependancy tree\n",
    "spacy.displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Bass, lacking)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Zero RULE \n",
    "## Noun - Adjective pairs\n",
    "\n",
    "## Very basic rule. Should be least weightage\n",
    "\n",
    "noun_adj_pairs = []\n",
    "for i,token in enumerate(doc):\n",
    "    if token.pos_ not in ('NOUN','PROPN'):\n",
    "        continue\n",
    "    for j in range(i+1,len(doc)):\n",
    "        if doc[j].pos_ == 'ADJ':\n",
    "            noun_adj_pairs.append((token,doc[j]))\n",
    "            break\n",
    "noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quality', 'sound'),\n",
       " ('lighting', 'low'),\n",
       " ('front', 'poor'),\n",
       " ('cameras', 'front')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FIRST RULE OF DEPENDANCY PARSE -\n",
    "## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "## RULE = M is child of A with a relationshio of amod\n",
    "\n",
    "rule1_pairs = []\n",
    "for token in doc:\n",
    "    if token.dep_ == \"amod\":\n",
    "        rule1_pairs.append((token.head.text, token.text))\n",
    "\n",
    "rule1_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SECOND RULE OF DEPENDANCY PARSE -\n",
    "## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#Direct Object - A is a child of something with relationship of nsubj, while \n",
    "# M is a child of the same something with relationship of dobj\n",
    "\n",
    "#Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "rule2_pairs = []\n",
    "for token in doc:\n",
    "    children = token.children\n",
    "    A = \"999999\"\n",
    "    M = \"999999\"\n",
    "    for child in children :\n",
    "        if(child.dep_ == \"nsubj\"):\n",
    "            A = child.text\n",
    "        if(child.dep_ == \"dobj\"):\n",
    "            M = child.text\n",
    "    if(A != \"999999\" and M != \"999999\"):\n",
    "        rule2_pairs.append((A, M))   \n",
    "    \n",
    "         \n",
    "            \n",
    "        \n",
    "        \n",
    "rule2_pairs        \n",
    "        \n",
    "        \n",
    "   # if token.dep_ == \"amod\":\n",
    "   #     rule1_pairs.append((token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quality', 'wonderful'), ('packaging', 'better'), ('Photos', 'better')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THIRD RULE OF DEPENDANCY PARSE -\n",
    "## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#Adjectival Complement - A is a child of something with relationship of nsubj, while \n",
    "# M is a child of the same something with relationship of acomp\n",
    "\n",
    "#Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "rule3_pairs = []\n",
    "for token in doc:\n",
    "\n",
    "    children = token.children\n",
    "    A = \"999999\"\n",
    "    B = \"999999\"\n",
    "    for child in children :\n",
    "        if(child.dep_ == \"nsubj\"):\n",
    "            A = child.text\n",
    "          \n",
    "        if(child.dep_ == \"acomp\"):\n",
    "            M = child.text\n",
    "        \n",
    "    if(A != \"999999\" and M != \"999999\"):\n",
    "        rule3_pairs.append((A, M)) \n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "rule3_pairs        \n",
    "        \n",
    "        \n",
    "   # if token.dep_ == \"amod\":\n",
    "   #     rule1_pairs.append((token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while \n",
    "# M is a child of the same something with relationship of advmod\n",
    "\n",
    "#Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "rule4_pairs = []\n",
    "for token in doc:\n",
    "\n",
    "    children = token.children\n",
    "    A = \"999999\"\n",
    "    M = \"999999\"\n",
    "    for child in children :\n",
    "        if(child.dep_ == \"nsubjpass\"):\n",
    "            A = child.text\n",
    "          \n",
    "        if(child.dep_ == \"advmod\"):\n",
    "            M = child.text\n",
    "        \n",
    "    if(A != \"999999\" and M != \"999999\"):\n",
    "        rule4_pairs.append((A, M)) \n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "rule4_pairs        \n",
    "        \n",
    "        \n",
    "   # if token.dep_ == \"amod\":\n",
    "   #     rule1_pairs.append((token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FIFTH RULE OF DEPENDANCY PARSE -\n",
    "## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#Complement of a copular verb - A is a child of M with relationship of nsubj, while \n",
    "# M has a child with relationship of cop\n",
    "\n",
    "#Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "rule5_pairs = []\n",
    "for token in doc:\n",
    "    children = token.children\n",
    "    A = \"999999\"\n",
    "    buf_var = \"999999\"\n",
    "    for child in children :\n",
    "        if(child.dep_ == \"nsubj\"):\n",
    "            A = child.text\n",
    "          \n",
    "        if(child.dep_ == \"cop\"):\n",
    "            buf_var = child.text\n",
    "        \n",
    "    if(A != \"999999\" or buf_var != \"999999\"):\n",
    "        rule3_pairs.append((A, token.text)) \n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "rule5_pairs        \n",
    "        \n",
    "        \n",
    "   # if token.dep_ == \"amod\":\n",
    "   #     rule1_pairs.append((token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quality', 'wonderful'), ('packaging', 'better'), ('Photos', 'better')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule3_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aspects = []\n",
    "aspects = rule1_pairs + rule2_pairs + rule3_pairs + rule4_pairs + rule5_pairs \n",
    "aspects\n",
    "\n",
    "review_id = \"guhjsus23\"\n",
    "\n",
    "{\"review_id\" : review_id, \"aspects\" : aspects }\n",
    "aspects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extraction(row,nlp):\n",
    "    review_body = row['review_body']\n",
    "    review_id = row['review_id']\n",
    "\n",
    "    doc=nlp(review_body)\n",
    "\n",
    "\n",
    "    ## FIRST RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    ## RULE = M is child of A with a relationshio of amod\n",
    "    rule1_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"amod\":\n",
    "            rule1_pairs.append((token.head.text, token.text))\n",
    "            #return row['height'] * row['width']\n",
    "\n",
    "\n",
    "    ## SECOND RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Direct Object - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of dobj\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule2_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "            if(child.dep_ == \"dobj\"):\n",
    "                M = child.text\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule2_pairs.append((A, M))\n",
    "\n",
    "\n",
    "    ## THIRD RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of acomp\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule3_pairs = []\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"acomp\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule3_pairs.append((A, M))\n",
    "\n",
    "    ## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
    "    # M is a child of the same something with relationship of advmod\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule4_pairs = []\n",
    "    for token in doc:\n",
    "\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubjpass\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"advmod\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule4_pairs.append((A, M))\n",
    "\n",
    "\n",
    "    ## FIFTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
    "    # M has a child with relationship of cop\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule5_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        buf_var = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"cop\"):\n",
    "                buf_var = child.text\n",
    "\n",
    "        if(A != \"999999\" and buf_var != \"999999\"):\n",
    "            rule3_pairs.append((A, token.text))\n",
    "\n",
    "    aspects = []\n",
    "    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs\n",
    "    dic = {\"review_id\" : review_id , \"aspect_pairs\" : aspects}\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'R372S58V6D11AT', 'aspect_pairs': [('Bass', 'lacking')]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_lg\")\n",
    "reviews.apply(lambda row: apply_extraction(row,nlp), axis=1)[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva",
   "language": "python",
   "name": "dva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
