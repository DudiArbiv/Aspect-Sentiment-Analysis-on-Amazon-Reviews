{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "BASE_PATH = os.path.abspath(os.path.join('..')) # base path of project\n",
    "\n",
    "file_path = BASE_PATH + '/data/raw/amazon_reviews_us_Electronics_v1_00.tsv'\n",
    "\n",
    "raw_data = pd.read_table(file_path,error_bad_lines=False, nrows=100)\n",
    "\n",
    "#review_body = raw_data['review_body']\n",
    "reviews = raw_data[['review_id', 'review_body']]\n",
    "nlp=spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extraction(row,nlp):\n",
    "    review_body = row['review_body']\n",
    "    review_id = row['review_id']\n",
    "\n",
    "    doc=nlp(review_body)\n",
    "\n",
    "\n",
    "    ## FIRST RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    ## RULE = M is child of A with a relationshio of amod\n",
    "    rule1_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"amod\":\n",
    "            rule1_pairs.append((token.head.text, token.text))\n",
    "            #return row['height'] * row['width']\n",
    "\n",
    "\n",
    "    ## SECOND RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Direct Object - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of dobj\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule2_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "            if(child.dep_ == \"dobj\"):\n",
    "                M = child.text\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule2_pairs.append((A, M))\n",
    "\n",
    "\n",
    "    ## THIRD RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of acomp\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule3_pairs = []\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"acomp\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule3_pairs.append((A, M))\n",
    "\n",
    "    ## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
    "    # M is a child of the same something with relationship of advmod\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule4_pairs = []\n",
    "    for token in doc:\n",
    "\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubjpass\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"advmod\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            rule4_pairs.append((A, M))\n",
    "\n",
    "\n",
    "    ## FIFTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
    "    # M has a child with relationship of cop\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule5_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        buf_var = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"cop\"):\n",
    "                buf_var = child.text\n",
    "\n",
    "        if(A != \"999999\" and buf_var != \"999999\"):\n",
    "            rule3_pairs.append((A, token.text))\n",
    "\n",
    "    aspects = []\n",
    "    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs\n",
    "    dic = {\"review_id\" : review_id , \"aspect_pairs\" : aspects}\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'R372S58V6D11AT', 'aspect_pairs': [('Bass', 'lacking')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_decomp = reviews.apply(lambda row: apply_extraction(row,nlp), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "import spacy\n",
    "#from spacy import displacy\n",
    "from sklearn import cluster \n",
    "from collections import defaultdict\n",
    "\n",
    "#from src/models import aspect_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src/models import aspect_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_decomp = [{\"aspect_pairs\": [(\"sound\", \"great\"),(\"music\", \"loud\"),(\"shipping\", \"poor\")]},\n",
    "                 {\"aspect_pairs\": [(\"packaging\", \"pathetic\"),(\"sound\", \"perfect\")]},\n",
    "                 {\"aspect_pairs\": [(\"quality\", \"wonderful\"),(\"photos\", \"great\")]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_lg')  # make sure to use larger model!\n",
    "#tokens = nlp(u'sound music shipping packaging bass')\n",
    "#for token1 in tokens:\n",
    "#    for token2 in tokens:\n",
    "  #      print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = []\n",
    "for review in review_decomp:\n",
    "    aspect_pairs = review[\"aspect_pairs\"]\n",
    "    for noun, adj in aspect_pairs:\n",
    "        aspects.append(noun)\n",
    "\n",
    "unique_aspects = list(set(aspects)) # use this list for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this mapping later for tagging clusters\n",
    "aspects_map = defaultdict(int) \n",
    "for asp in aspects:\n",
    "    aspects_map[asp] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')  # make sure to use larger model!\n",
    "\n",
    "asp_vectors = []\n",
    "for aspect in unique_aspects:\n",
    "    token = nlp(aspect)\n",
    "    asp_vectors.append(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(asp_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id labels for inputted data\n",
      "['sound', 'music', 'packaging', 'photos', 'shipping', 'quality']\n",
      "[1 1 0 1 0 0]\n",
      "{'sound': 1, 'music': 1, 'packaging': 0, 'photos': 1, 'shipping': 0, 'quality': 0}\n"
     ]
    }
   ],
   "source": [
    "#centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_ \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (unique_aspects)\n",
    "print (labels)\n",
    "\n",
    "asp_to_cluster_map = dict(zip(unique_aspects,labels))\n",
    "print(asp_to_cluster_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['packaging', 'shipping', 'quality']\n",
      "{'shipping': 1, 'packaging': 1, 'quality': 1}\n",
      "[('shipping', 1), ('packaging', 1), ('quality', 1)]\n",
      "['sound', 'music', 'photos']\n",
      "{'sound': 2, 'music': 1, 'photos': 1}\n",
      "[('music', 1), ('photos', 1), ('sound', 2)]\n",
      "defaultdict(None, {0: 'shipping', 1: 'music'})\n"
     ]
    }
   ],
   "source": [
    "cluster_map = defaultdict()\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_nouns = [k for k,v in asp_to_cluster_map.items() if v == i]\n",
    "#     print(cluster_nouns)\n",
    "    freq_map = {k:v for k,v in aspects_map.items() if k in cluster_nouns}\n",
    "    freq_map = sorted(freq_map.items(), key = lambda x: x[1])\n",
    "#     print(freq_map)\n",
    "    cluster_map[i] = freq_map[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
